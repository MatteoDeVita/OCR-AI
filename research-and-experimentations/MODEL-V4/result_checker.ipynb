{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6692b33cb4e8400fba547d049946d770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/502 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "502\n",
      "25\n",
      "[' ', \"'\", '0', '1', '2', '7', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Model: \"handwriting_recognizer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " image (InputLayer)             [(None, 256, 64, 1)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " Conv1 (Conv2D)                 (None, 256, 64, 128  1280        ['image[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool1 (MaxPooling2D)           (None, 128, 32, 128  0           ['Conv1[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " Conv2 (Conv2D)                 (None, 128, 32, 128  147584      ['pool1[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " pool2 (MaxPooling2D)           (None, 64, 16, 128)  0           ['Conv2[0][0]']                  \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 64, 2048)     0           ['pool2[0][0]']                  \n",
      "                                                                                                  \n",
      " dense1 (Dense)                 (None, 64, 128)      262272      ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 64, 128)      0           ['dense1[0][0]']                 \n",
      "                                                                                                  \n",
      " bidirectional_4 (Bidirectional  (None, 64, 256)     263168      ['dropout_2[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " bidirectional_5 (Bidirectional  (None, 64, 256)     394240      ['bidirectional_4[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " label (InputLayer)             [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " dense2 (Dense)                 (None, 64, 61)       15677       ['bidirectional_5[0][0]']        \n",
      "                                                                                                  \n",
      " ctc_loss (CTCLayer)            (None, 64, 61)       0           ['label[0][0]',                  \n",
      "                                                                  'dense2[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,084,221\n",
      "Trainable params: 1,084,221\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(data_file_path, len_range, max_space, max_len=-1):\n",
    "    dataset = []\n",
    "    lines = open(data_file_path, \"r\").readlines()\n",
    "    if max_len != -1:\n",
    "        lines = lines[:max_len]\n",
    "    for line in tqdm(lines):\n",
    "        splitted_line = line.split(' ', 1)\n",
    "        dataset.append({\n",
    "            \"image_path\": splitted_line[0],\n",
    "            \"label\": splitted_line[1].split('\\n')[0]\n",
    "        })\n",
    "    # dataset = list(filter(lambda data: filter_data(data, len_range, max_space), dataset))\n",
    "    return dataset\n",
    "\n",
    "dataset = load_dataset(\"./data.txt\", len_range=(3, 32), max_space=3, max_len=582_103*2)\n",
    "\n",
    "print(len(dataset))\n",
    "# For computer vision deep learning, there is a consensus saying that a dataset of 1000 labeled images for each classes is needed\n",
    "image_paths = list(map(lambda data: data[\"image_path\"], dataset))\n",
    "labels = list(map(lambda data: data[\"label\"].replace('|',  '\\n'), dataset))\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "train_ds = dataset[:int(0.05*len(dataset))] #98% of the whole dataset is train dataset\n",
    "validation_ds = dataset[int(0.05*len(dataset)):int(0.1*len(dataset))] #1% is  validation dataset\n",
    "test_ds = dataset[int(0.1*len(dataset)):] #1% is test dataset\n",
    "\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE # Let tf decide the best tunning algos\n",
    "\n",
    "characters = sorted(list(set(char for label in labels for char in label)))\n",
    "max_len = len(max(labels, key=len))\n",
    "print(max_len)\n",
    "print(characters)\n",
    "# Mapping characters to integer -> returns a function\n",
    "char_to_num = StringLookup(vocabulary=list(characters), mask_token=None)\n",
    "\n",
    "# Mapping integers back to original characters -> returns a function\n",
    "num_to_char = StringLookup(vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True)\n",
    "\n",
    "batch_size = 64\n",
    "padding_token = 99\n",
    "image_height = 64\n",
    "image_width = image_height * 4\n",
    "\n",
    "def distortion_free_resize(image, img_size):\n",
    "    w, h = img_size\n",
    "    image = tf.image.resize(image, size=(h, w), preserve_aspect_ratio=True)\n",
    "\n",
    "    # Check tha amount of padding needed to be done.\n",
    "    pad_height = h - tf.shape(image)[0]\n",
    "    pad_width = w - tf.shape(image)[1]\n",
    "\n",
    "    # Only necessary if you want to do same amount of padding on both sides.\n",
    "    if pad_height % 2 != 0:\n",
    "        height = pad_height // 2\n",
    "        pad_height_top = height + 1\n",
    "        pad_height_bottom = height\n",
    "    else:\n",
    "        pad_height_top = pad_height_bottom = pad_height // 2\n",
    "\n",
    "    if pad_width % 2 != 0:\n",
    "        width = pad_width // 2\n",
    "        pad_width_left = width + 1\n",
    "        pad_width_right = width\n",
    "    else:\n",
    "        pad_width_left = pad_width_right = pad_width // 2\n",
    "\n",
    "    image = tf.pad(\n",
    "        image,\n",
    "        paddings=[\n",
    "            [pad_height_top, pad_height_bottom],\n",
    "            [pad_width_left, pad_width_right],\n",
    "            [0, 0],\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    image = tf.transpose(image, perm=[1, 0, 2])\n",
    "    image = tf.image.flip_left_right(image)\n",
    "    return image\n",
    "\n",
    "def preprocess_image(image_path, img_size=(image_width, image_height)):\n",
    "    image = tf.io.read_file(image_path) # Open file with tf\n",
    "    image = tf.image.decode_png(image, channels=1) # transform to matrix of gray scale value\n",
    "    image = distortion_free_resize(image, img_size) # Distort image\n",
    "    image = tf.cast(image, tf.float32) / 255.0 # Transform image to data into matrix of gray scale float32 values in range [0, 1]\n",
    "    return image\n",
    "\n",
    "def vectorize_label(label):\n",
    "    label = char_to_num(tf.strings.unicode_split(label, input_encoding=\"UTF-8\"))\n",
    "    length = tf.shape(label)[0]\n",
    "    pad_amount = max_len - length\n",
    "    label = tf.pad(label, paddings=[[0, pad_amount]], constant_values=padding_token)\n",
    "    return label\n",
    "\n",
    "def process_images_labels(image_path, label):\n",
    "    image = preprocess_image(image_path)\n",
    "    label = vectorize_label(label)\n",
    "    return {\"image\": image, \"label\": label}\n",
    "\n",
    "def prepare_dataset(image_paths, labels):\n",
    "    return tf.data.Dataset.from_tensor_slices(\n",
    "        (image_paths, labels)\n",
    "    ).map(\n",
    "        process_images_labels, num_parallel_calls=AUTOTUNE\n",
    "    ).batch(batch_size)\n",
    "    # return tf.data.Dataset.from_tensor_slices(\n",
    "    #     (image_paths, labels)\n",
    "    # ).map(\n",
    "    #     process_images_labels, num_parallel_calls=AUTOTUNE\n",
    "    # ).batch(batch_size).cache().prefetch(AUTOTUNE)\n",
    "\n",
    "train_ds = prepare_dataset(list(map(lambda data: data[\"image_path\"], train_ds)), list(map(lambda data: data[\"label\"], train_ds)))\n",
    "validation_ds = prepare_dataset(list(map(lambda data: data[\"image_path\"], validation_ds)), list(map(lambda data: data[\"label\"], validation_ds)))\n",
    "test_ds = prepare_dataset(list(map(lambda data: data[\"image_path\"], test_ds)), list(map(lambda data: data[\"label\"], test_ds)))\n",
    "\n",
    "\n",
    "class CTCLayer(keras.layers.Layer):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.loss_fn = keras.backend.ctc_batch_cost\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "\n",
    "        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n",
    "        self.add_loss(loss)\n",
    "        \n",
    "        # At test time, just return the computed predictions.\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    # Inputs to the model\n",
    "    input_img = keras.Input(shape=(image_width, image_height, 1), name=\"image\")\n",
    "    labels = keras.layers.Input(name=\"label\", shape=(None,))\n",
    "\n",
    "    # First conv block.\n",
    "    x = keras.layers.Conv2D(\n",
    "        128,\n",
    "        (3, 3),\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        padding=\"same\",\n",
    "        name=\"Conv1\",\n",
    "    )(input_img)\n",
    "    x = keras.layers.MaxPooling2D((2, 2), name=\"pool1\")(x)\n",
    "\n",
    "    # Second conv block.\n",
    "    x = keras.layers.Conv2D(\n",
    "        128,\n",
    "        (3, 3),\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        padding=\"same\",\n",
    "        name=\"Conv2\",\n",
    "    )(x)\n",
    "    x = keras.layers.MaxPooling2D((2, 2), name=\"pool2\")(x)\n",
    "\n",
    "    # We have used two max pool with pool size and strides 2.\n",
    "    # Hence, downsampled feature maps are 4x smaller. The number of\n",
    "    # filters in the last layer is 64. Reshape accordingly before\n",
    "    # passing the output to the RNN part of the model.\n",
    "    new_shape = ((image_width // 4), (image_height // 4) * 128)\n",
    "    x = keras.layers.Reshape(target_shape=new_shape, name=\"reshape\")(x)\n",
    "    x = keras.layers.Dense(128, activation=\"relu\", name=\"dense1\")(x)\n",
    "    x = keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "    # RNNs.\n",
    "    x = keras.layers.Bidirectional(\n",
    "        keras.layers.LSTM(128, return_sequences=True, dropout=0.25)\n",
    "    )(x)\n",
    "    x = keras.layers.Bidirectional(\n",
    "        keras.layers.LSTM(128, return_sequences=True, dropout=0.25)\n",
    "    )(x)\n",
    "\n",
    "    # +2 is to account for the two special tokens introduced by the CTC loss.\n",
    "    # The recommendation comes here: https://git.io/J0eXP.\n",
    "    x = keras.layers.Dense(\n",
    "        len(char_to_num.get_vocabulary()) + 2, activation=\"softmax\", name=\"dense2\"\n",
    "    )(x)\n",
    "\n",
    "    # Add CTC layer for calculating CTC loss at each step.\n",
    "    output = CTCLayer(name=\"ctc_loss\", )(labels, x)\n",
    "\n",
    "    # Define the model.\n",
    "    model = keras.models.Model(\n",
    "        inputs=[input_img, labels], outputs=output, name=\"handwriting_recognizer\"\n",
    "    )\n",
    "    # Optimizer.\n",
    "    # opt = keras.optimizers.Adam()\n",
    "    opt = keras.optimizers.Adam()\n",
    "    # Compile the model and return.\n",
    "    model.compile(optimizer=opt)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Get the model.\n",
    "model = build_model()\n",
    "#model.summary()\n",
    "######## EVALUATION METRICS\n",
    "validation_images = []\n",
    "validation_labels = []\n",
    "\n",
    "for batch in validation_ds:\n",
    "    validation_images.append(batch[\"image\"])\n",
    "    validation_labels.append(batch[\"label\"])\n",
    "\n",
    "def calculate_edit_distance(labels, predictions):\n",
    "    # Get a single batch and convert its labels to sparse tensors.\n",
    "    saprse_labels = tf.cast(tf.sparse.from_dense(labels), dtype=tf.int64)\n",
    "\n",
    "    # Make predictions and convert them to sparse tensors.\n",
    "    input_len = np.ones(predictions.shape[0]) * predictions.shape[1]\n",
    "    predictions_decoded = keras.backend.ctc_decode(\n",
    "        predictions, input_length=input_len, greedy=True\n",
    "    )[0][0][:, :max_len]\n",
    "    sparse_predictions = tf.cast(\n",
    "        tf.sparse.from_dense(predictions_decoded), dtype=tf.int64\n",
    "    )\n",
    "\n",
    "    # Compute individual edit distances and average them out.\n",
    "    edit_distances = tf.edit_distance(\n",
    "        sparse_predictions, saprse_labels, normalize=False\n",
    "    )\n",
    "    return tf.reduce_mean(edit_distances)\n",
    "\n",
    "\n",
    "class EditDistanceCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, pred_model):\n",
    "        super().__init__()\n",
    "        self.prediction_model = pred_model\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        edit_distances = []\n",
    "\n",
    "        for i in range(len(validation_images)):\n",
    "            labels = validation_labels[i]\n",
    "            predictions = self.prediction_model.predict(validation_images[i])\n",
    "            edit_distances.append(calculate_edit_distance(labels, predictions).numpy())\n",
    "\n",
    "        print(\n",
    "            f\"Mean edit distance for epoch {epoch + 1}: {np.mean(edit_distances):.4f}\"\n",
    "        )\n",
    "\n",
    "model.summary()\n",
    "model = build_model()\n",
    "prediction_model = keras.models.Model(\n",
    "    model.get_layer(name=\"image\").input, model.get_layer(name=\"dense2\").output\n",
    ")\n",
    "\n",
    "edit_distance_callback = EditDistanceCallback(prediction_model)\n",
    "\n",
    "early_stopping_patience = 10\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=early_stopping_patience, restore_best_weights=True)\n",
    "\n",
    "CP_PATH = \"./training2/cp-{epoch:04d}.ckpt\"\n",
    "TEMP_CP_PATH = \"TRAINING-ONLY-FONTS/cp-{epoch:04d}.ckpt\"\n",
    "\n",
    "# Create checkpoint\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=CP_PATH,\n",
    "    save_weights_only=True,\n",
    "    verbose=1,\n",
    "    # save_freq=5*batch_size #Only every 5 e^pochs\n",
    ")\n",
    "\n",
    "# Train the model.\n",
    "# history = model.fit(\n",
    "#     train_ds,\n",
    "#     validation_data=validation_ds,\n",
    "#     epochs=epochs,\n",
    "#     callbacks=[edit_distance_callback, cp_callback],\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Received incompatible tensor with shape (88,) when attempting to restore variable with shape (61,) and name layer_with_weights-5/bias/.ATTRIBUTES/VARIABLE_VALUE.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15148\\2662614783.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcheckpoint_path\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcheckpoint_paths\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./training4/cp-0015.ckpt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m     \u001b[0mdraw_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\saving\\saveable_object_util.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, restored_tensors, restored_shapes)\u001b[0m\n\u001b[0;32m    133\u001b[0m             self.handle_op, self._var_shape, restored_tensor)\n\u001b[0;32m    134\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m    136\u001b[0m             \u001b[1;34mf\"Received incompatible tensor with shape {restored_tensor.shape} \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m             \u001b[1;34mf\"when attempting to restore variable with shape {self._var_shape} \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Received incompatible tensor with shape (88,) when attempting to restore variable with shape (61,) and name layer_with_weights-5/bias/.ATTRIBUTES/VARIABLE_VALUE."
     ]
    }
   ],
   "source": [
    "checkpoint_paths = [\n",
    "    \"./training/cp-0001.ckpt\",\n",
    "    \"./training/cp-0002.ckpt\",\n",
    "    \"./training/cp-0003.ckpt\",\n",
    "    \"./training/cp-0004.ckpt\",\n",
    "    \"./training/cp-0005.ckpt\",\n",
    "    \"./training/cp-0006.ckpt\",\n",
    "    \"./training2/cp-0001.ckpt\",\n",
    "    \"./training2/cp-0002.ckpt\",\n",
    "    \"./training2/cp-0003.ckpt\",\n",
    "    \"./training2/cp-0004.ckpt\",\n",
    "    \"./training2/cp-0005.ckpt\",\n",
    "    \"./training2/cp-0006.ckpt\",\n",
    "    \"./training2/cp-0007.ckpt\",\n",
    "    \"./training2/cp-0008.ckpt\",\n",
    "    \"./training2/cp-0009.ckpt\",\n",
    "    \"./training3/cp-0001.ckpt\",\n",
    "    \"./training3/cp-0002.ckpt\",\n",
    "    \"./training3/cp-0003.ckpt\",\n",
    "    \"./training3/cp-0004.ckpt\",\n",
    "    \"./training3/cp-0005.ckpt\",\n",
    "    \"./training3/cp-0006.ckpt\",\n",
    "    \"./training3/cp-0007.ckpt\",\n",
    "    \"./training3/cp-0008.ckpt\",\n",
    "    \"./training4/cp-0001.ckpt\",\n",
    "    \"./training4/cp-0002.ckpt\",\n",
    "    \"./training4/cp-0003.ckpt\",\n",
    "    \"./training4/cp-0004.ckpt\",\n",
    "    \"./training4/cp-0005.ckpt\",\n",
    "    \"./training4/cp-0006.ckpt\",\n",
    "    \"./training4/cp-0007.ckpt\",\n",
    "    \"./training4/cp-0008.ckpt\",\n",
    "    \"./training4/cp-0009.ckpt\",\n",
    "    \"./training4/cp-0010.ckpt\",\n",
    "    \"./training4/cp-0011.ckpt\",\n",
    "    \"./training4/cp-0012.ckpt\",\n",
    "    \"./training4/cp-0013.ckpt\",\n",
    "    \"./training4/cp-0014.ckpt\",\n",
    "    \"./training4/cp-0015.ckpt\",\n",
    "    \"./training4/cp-0016.ckpt\",\n",
    "]\n",
    "\n",
    "\n",
    "# model = keras.models.load_model(\"./saved_model/CNN-MODEL-V4\")\n",
    "\n",
    "for checkpoint_path in checkpoint_paths:\n",
    "    model.load_weights(\"./training4/cp-0015.ckpt\")\n",
    "    draw_results()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
